{"cells":[{"cell_type":"markdown","source":["# T5を使用した学習"],"metadata":{"id":"zDUd4HOUVqrC"}},{"cell_type":"markdown","source":["## Google Driveとの連携"],"metadata":{"id":"WQNa6Hk8_BE1"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"CJc64wXG-8hZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ln -s \"/content/drive/My Drive/ColabNotebooks\" \"/content/ColabNotebooks\""],"metadata":{"id":"IkEd_M93A8A0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 必要パッケージのインストール"],"metadata":{"id":"x57jwoxnVvDd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LEzEE5lWdnuJ"},"outputs":[],"source":["!pip install -r /content/ColabNotebooks/requirements.txt"]},{"cell_type":"markdown","source":["## 学習"],"metadata":{"id":"r9NN07gmV19u"}},{"cell_type":"markdown","source":["### パッケージのimport"],"metadata":{"id":"2MCNArwhBZJE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_nRxsP_eZY3"},"outputs":[],"source":["import pandas as pd\n","import tokenizers\n","from transformers import (\n","    T5Tokenizer,\n","    T5ForConditionalGeneration,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n","    TrainerCallback,\n","    AdamW,\n","    get_linear_schedule_with_warmup,\n",")\n","import torch\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset\n","from evaluate import load as load_metric\n","import os\n","import math"]},{"cell_type":"markdown","source":["### 転移学習モデルのロード"],"metadata":{"id":"Cl9jeDaMi3RI"}},{"cell_type":"markdown","source":["#### モデルパスの定義"],"metadata":{"id":"T_yU12HmmC6M"}},{"cell_type":"code","source":["MODEL_SELECTIONS = (\n","    \"t5-small\",\n","    \"t5-base\",\n","    \"t5-large\",\n","    \"google/flan-t5-small\",\n","    \"google/flan-t5-base\",\n","    \"google/flan-t5-large\",\n","    \"google/flan-t5-xl\",\n","    \"google/flan-t5-xxl\",\n","    \"retrieva-jp/t5-small-short\",\n","    \"retrieva-jp/t5-small-medium\",\n","    \"retrieva-jp/t5-small-long\",\n","    \"retrieva-jp/t5-base-short\",\n","    \"retrieva-jp/t5-base-medium\",\n","    \"retrieva-jp/t5-base-long\",\n","    \"retrieva-jp/t5-large-short\",\n","    \"retrieva-jp/t5-large-medium\",\n","    \"retrieva-jp/t5-large-long\",\n","    \"retrieva-jp/t5-xl\",\n","    \"sonoisa/t5-base-english-japanese\",\n","    \"sonoisa/t5-base-japanese\",\n","    \"sonoisa/t5-base-japanese-question-generation\",\n",")\n","model_path = MODEL_SELECTIONS[4]"],"metadata":{"id":"kNbt1cXHjBdS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### モデルの読み込み"],"metadata":{"id":"rlnz93WrmIz9"}},{"cell_type":"code","source":["# トークナイザとモデルの準備\n","tokenizer = T5Tokenizer.from_pretrained(model_path)\n","model = T5ForConditionalGeneration.from_pretrained(model_path)"],"metadata":{"id":"7ooTk8DHjb0-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 固定値の設定"],"metadata":{"id":"yVe3qchCjzO2"}},{"cell_type":"code","source":["# 生成元のシーケンスの最大長を定義します。\n","max_length_src = 32\n","\n","# 生成されるシーケンスの最大長を定義します。\n","# これは問題の性質によって異なりますが、\n","# 一般的にはソーステキストの長さの2倍程度を指定することが推奨されます。\n","max_length_target = 64\n","\n","# エポックとは、全ての訓練データがネットワークを通過する回数を指します。\n","# この値はモデルが過学習しないようにするために調整する必要があります。\n","# 一般的には、数エポック（2〜10など）から始め、\n","# 過学習の兆候を見つけるために検証セットのパフォーマンスを監視します。\n","num_train_epochs = 3\n","\n","# 各デバイス（GPUまたはCPU）での訓練バッチサイズです。\n","# バッチサイズとは、同時にネットワークを通過するトレーニングデータの数を指します。\n","# この値は、利用可能なメモリに応じて適切に調整する必要があります。\n","# 一般的には、8から32の範囲が適切な値とされています。\n","per_device_train_batch_size=16\n","\n","# 各デバイス（GPUまたはCPU）での評価バッチサイズです。\n","# 訓練バッチサイズ同様、利用可能なメモリに応じて調整する必要があります。\n","# 一般的には、訓練バッチサイズと同等またはそれ以上の値が選択されます。\n","per_device_eval_batch_size=8\n","\n","# ウォームアップステップの数は、学習率スケジューリングに影響します。\n","# 学習の初期段階で学習率を徐々に上げることで、訓練の安定性を向上させることができます。\n","# 適切な値は訓練ステップの総数の一部で、\n","# 一般的にはトータルステップの10%程度が指定されます。\n","warmup_steps=1000\n","\n","# 重み減衰は正則化手法の一つで、モデルの複雑さを制限することで過学習を防ぎます。\n","# 一般的には0.0から0.01の範囲が推奨されます。\n","weight_decay=0.01\n","\n","# 学習率はモデルのパラメータ更新のスピードを制御します。\n","# 大きすぎると学習が不安定になり、小さすぎると学習が遅くなります。\n","# 一般的には0.00001（1e-5）から0.1の範囲で設定します。\n","learning_rate=5e-5\n","\n","# 勾配累積ステップ数は、バッチサイズを事実上増加させるために使用されます。\n","# これにより、ハードウェアのメモリ制限に直面している場合でも、大きなバッチで訓練を行うことができます。\n","# 適切な値はハードウェアのメモリとバッチサイズに依存します。\n","# 例えば、バッチサイズが8で、メモリがそれ以上のバッチサイズを扱うことができない場合、\n","# 勾配累積ステップ数を2に設定することで、実質的なバッチサイズを16にすることができます。\n","# ただし、勾配累積は計算時間を増加させる可能性があります。\n","gradient_accumulation_steps=1\n","\n","prefix = \"translate English to Japanese: \"\n","# prefix = \"translate English to French: \""],"metadata":{"id":"R2muz3X1j5Li"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 関連クラス・メソッドの定義"],"metadata":{"id":"Dj6edFAIB1GK"}},{"cell_type":"markdown","source":["#### データセット用のクラスの作成"],"metadata":{"id":"_UQUbx8jmUhW"}},{"cell_type":"code","source":["class WordTranslationDataset(Dataset):\n","    def __init__(self, tokenizer, df):\n","        self.tokenizer = tokenizer\n","        self.df = df\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        english_word = prefix + row[\"English\"]\n","        wrong_translation = row[\"Japanese\"]\n","        encoding = self.tokenizer.encode_plus(\n","            english_word,\n","            max_length=max_length_src,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors=\"pt\",\n","        )\n","        targets = self.tokenizer.encode_plus(\n","            wrong_translation,\n","            max_length=max_length_target,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors=\"pt\",\n","        )\n","        labels = targets[\"input_ids\"].flatten()\n","        labels[labels == self.tokenizer.pad_token_id] = -100\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].flatten(),\n","            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n","            \"labels\": labels,\n","        }"],"metadata":{"id":"hzXFIwI_BjYd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### オプチマイザ、スケジューラー作成用メソッド"],"metadata":{"id":"gJizG4iymgWP"}},{"cell_type":"code","source":["def get_optimizers(training_steps):\n","  optimizer = AdamW(model.parameters(), lr=learning_rate)\n","  # 線形に減衰するスケジューラー\n","  scheduler = get_linear_schedule_with_warmup(\n","      optimizer,\n","      num_warmup_steps=warmup_steps,\n","      num_training_steps=training_steps,\n","  )\n","  return optimizer, scheduler"],"metadata":{"id":"X2LLfq67msth"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 評価用メソッド"],"metadata":{"id":"eOEB7Ug3m00h"}},{"cell_type":"code","source":["# BLEUメトリクスの読み込み\n","bleu_metric = load_metric(\"bleu\")\n","\n","# BLEUスコアでの検証を行う\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(\n","        predictions, skip_special_tokens=True\n","    )\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    bleu_score = bleu_metric.compute(\n","        predictions=decoded_preds, references=decoded_labels\n","    )\n","    print(f\"bleu_score: {bleu_score['score']}\")\n","\n","    return {\"bleu_score\": bleu_score[\"score\"]}"],"metadata":{"id":"g_fiqAuIm5XK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 学習の実行"],"metadata":{"id":"T9BBAzj1C3Eh"}},{"cell_type":"code","source":["# データの読み込みと前処理\n","# 英単語と誤った日本語訳のペアのcsv\n","df = pd.read_csv('/content/ColabNotebooks/input/en_and_ja.csv')\n","train_df, eval_df = train_test_split(df, test_size=0.2)\n","\n","# データセットの作成\n","train_dataset = WordTranslationDataset(tokenizer, train_df)\n","eval_dataset = WordTranslationDataset(tokenizer, eval_df)\n","\n","### GPUの利用有無\n","USE_GPU = torch.cuda.is_available()\n","if USE_GPU:\n","    model.cuda()\n","\n","# 最大ステップ数を計算(最低１回)\n","max_steps = max(\n","    1,\n","    math.ceil(len(train_dataset) / per_device_train_batch_size)\n","    * num_train_epochs,\n",")\n","\n","# トレーニングの設定\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir='results',  \n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,  \n","    per_device_eval_batch_size=per_device_eval_batch_size,\n","    weight_decay=weight_decay,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    logging_dir='logs',        \n",")\n","\n","optimizers = get_optimizers(max_steps)\n","\n","# トレーナーの作成\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# 訓練\n","trainer.train()\n","\n","# 訓練が終了したモデルとトークナイザを保存\n","model.save_pretrained(\"/content/ColabNotebooks/model/\")\n","tokenizer.save_pretrained('/content/ColabNotebooks/model/')"],"metadata":{"id":"sCoa_uvDBSGb"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["WQNa6Hk8_BE1","x57jwoxnVvDd","2MCNArwhBZJE","Cl9jeDaMi3RI","T_yU12HmmC6M","rlnz93WrmIz9","yVe3qchCjzO2","Dj6edFAIB1GK","_UQUbx8jmUhW","gJizG4iymgWP","eOEB7Ug3m00h"],"mount_file_id":"1I-CGm9eq8Fs5gkNsNfop3FY7VObNFiWR","authorship_tag":"ABX9TyMO+jYlGhWA5aaKDvIPSrqV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}