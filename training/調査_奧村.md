# 調査
## 目的
- 英単語を入力として受け取り、その単語の正しい日本語訳とは異なる、しかし合理的な日本語訳を返す「意味的類義語生成モデル」を作成する
## 手法一覧
### ルールベース
- 明確な規則やパターンに基づいて言語を解析する手法
- 特定のタスクに対してカスタマイズされた規則を作成することができ、その結果を完全に制御することができる
#### 欠点
- スケーラビリティに欠け、新しいデータやコンテキストに対して柔軟性がない
- 規則を作成するためには大量の手作業と専門知識が必要
### ナイーブベイズ、SVM
- 統計的なモデル、データから直接学習を行う手法
- ルールベースのモデルよりも柔軟性がある
- 理解しやすいアルゴリズムであり、モデルの挙動を解釈することが比較的容易
#### 欠点
- データの表面的な特徴に依存する学習を行うため、言語の深い構造や意味を捉えることが難しい
### RNN、LSTM、CNN
- ニューラルネットワークベースのモデル
- 深層学習を利用して、データの複雑なパターンを捉えることが可能
- RNNとLSTMは時系列データの処理に優れている
- CNNはローカルな特徴を効率的に捉えることが可能
#### 欠点
- 長い依存関係を効果的にモデリングすることが難しい
  - LSTMは一定程度解決しますが、それでも限界があります
- モデルの訓練には大量のデータと計算リソースが必要
### **BERT, GPT, T5**
- Transformerベースのモデル
#### 自己注意メカニズム
- モデルは文中のすべての単語間の関係性を同時に考慮でき、長い範囲の依存関係を効果的に捉えることが可能
#### 並列計算
- RNNやLSTMとは異なり、全ての位置の単語を一度に処理するため、計算を並列化することが可能
- モデルの訓練速度が大幅に向上している
#### スケーラビリティ
- モデルの深さ（レイヤー数）や幅（隠れ層のサイズ）を増やすことで容易にスケーリングすることが可能
- 大量のデータを効率的に利用する能力がある
#### 転移学習
- 一度学習した知識を別のタスクに転移することが可能
- 一部のタスクでは少量のデータでも高いパフォーマンスを達成することが可能
## ライブラリ一覧
###  NLTK (Natural Language Toolkit)
- Pythonの自然言語処理ライブラリで、テキスト処理ツールの豊富なコレクションを提供
- 語彙解析、形態素解析、構文解析、意味解析など、基本的な言語処理機能が揃っている
- 教育や研究にもよく使用される
#### 欠点
- 深層学習モデルに対する直接のサポートは限定的
- 最新のトランスフォーマーモデルを扱うのは難しい
### spaCy
- 産業向けの自然言語処理ライブラリで、高速で正確な言語解析を提供
- 名前付きエンティティ認識（NER）、品詞タグ付け、依存関係解析など、多くの高度な機能が備わっている
#### 欠点
- sある程度の自己学習機能を持ってるが、大規模なトランスフォーマーモデルへのサポートは限定的
### TensorFlow/ Keras
- 一般的な深層学習ライブラリで、強力な自カスタマイズ機能を備えている
- ニューラルネットワークの構造を自由に設計し、最適化、損失関数などを自由に選択可能
#### 欠点
- 特にNLPのタスクに対して、モデルをゼロから設計・訓練する必要があるため、時間とリソースが大量に必要
### **Transformers**
#### 事前学習済みモデル
- 大量のテキストデータで訓練されている、多数の事前学習済みモデル（BERT, GPT-2, T5, RoBERTaなど）を提供
- 自分でゼロからモデルを訓練する必要がなく、リソースと時間を節約可能
#### 微調整が容易
- 事前学習済みモデルを独自のタスクに合わせて微調整することが容易
#### 柔軟性と相互運用性
- PyTorchとTensorFlowの両方のフレームワークで動作可能
#### コミュニティサポート
- 活発なコミュニティによってサポートされており、新しいモデルや機能が定期的に追加されている
- 質問や問題に対する支援も充実している
## Transformerベースのモデル一覧
### GPT（Generative Pretrained Transformer）
- 強力な生成モデルで、自然で流暢なテキストを生成する能力がある
- 文章生成や文章補完などのタスクに有効
#### 欠点
- 一方向（左から右）の文脈しか考慮しないため、文脈を総合的に理解する能力は低い
### **T5 (Text-to-Text Transfer Transformer)**
- 全てのNLPタスクをテキスト生成の問題として扱う
- さまざまなタスク間で学習した知識を共有し、転移学習を効率的に行うことができる
#### 欠点
- 非常に大規模なモデルであるため、訓練や推論に多くの計算リソースが必要
### Transformer-XL
- 長いシーケンスに対する依存関係をモデリングする能力がある
#### 欠点
- 比較的複雑なモデルであり、大量の訓練データと計算リソースが必要
- 短い依存関係しか持たないタスクや、シーケンスの長さがそれほど重要でないタスクでは性能が高くない
### **BERT（Bidirectional Encoder Representations from Transformers）**
#### 文脈理解
- 左右の文脈を考慮して単語の意味を理解するため、単語の意味をより正確に捉えることが可能
#### 事前学習と微調整
- 大量のテキストデータで事前に学習され、特定のタスクに対して微調整することが可能
- タスク固有のデータが少ない場合でも高いパフォーマンスを発揮できる
#### 多言語対応
- 多言語版も存在し、英語だけでなく日本語などの他の言語の単語も理解することが可能
## 結論
- ~~学習モデルは、**BERT**を使用する~~
- 学習モデルは、**T5**を使用する
- ライブラリは、**Transformers**を使用する
###　理由
- BERTは双方向のTransformerを使用しており、モデルは単語の前後の文脈を同時に捉えることができます。
本タスクでは、英単語単体の意味を学習することが主目的で、特に文脈に依存する情報は少ないですが、BERTが持つ深い文脈理解の能力は、より高度な意味解釈や多義性の解消に有用です。
- GPTのようなモデルは、文生成タスクに強いですが、本タスクでは生成ではなく、翻訳の適切性に関する判定が求めらておりますので、適しておりません
- Transformer-XLは、長いシーケンスに対する依存関係をモデリングする能力が強みです。しかし、今回のタスクは個々の単語とその翻訳を学習するものであり、長い文脈の依存関係を捉える必要がないです。
- ~~今回のタスクは単純な翻訳生成タスクであり、T5は性能が過剰です~~
- BERTは主に単語や文の埋め込み（embedding）を学習するために使用され、その後のタスク（例えば、文書分類、名前付きエンティティ認識、質問応答など）に対してこれらの埋め込みを活用します。しかし、BERTは直接的にはシーケンス間の変換（sequence-to-sequence）タスクに使用することはできません。
- T5（Text-to-Text Transfer Transformer）は、シーケンス間の変換タスクを直接的に扱うために設計されています。このため、シーケンス間の変換タスク、例えば翻訳や文章の要約などに適しています。
#### BARTで行う場合
- 英単語とその誤った日本語訳を一緒に1つの入力シーケンスとしてBERTに提供し、モデルがこのペアが正しいか誤っているかを判定するように訓練する方法です。
このアプローチを使用する場合、学習データは英単語とその誤った日本語訳のペア、およびそのペアが正しいか誤っているかのラベル（この場合、すべてが誤っているとマークされている）を含む必要があります。評価は、新しい英単語とその誤った日本語訳のペアをモデルに提供し、モデルがどの程度それを誤っていると判断するかに基づいて行います。
ただし、このアプローチではモデルが新しい誤った日本語訳を生成することはできません。代わりに、モデルは提供された英単語と日本語訳のペアがどの程度誤っているかを評価します。
## 実装アプローチ一覧
### ノイズ付き訓練
訓練データの日本語訳にある程度のノイズ（すなわち、意図的な「誤訳」）を追加します。これにより、モデルは完全に正確な翻訳ではなく、ある程度のバリエーションを学習することができます。

### 多様なデコーディング戦略
ビームサーチやランダムサンプリングなどの異なるデコーディング戦略を試すことで、モデルの出力の多様性を増やすことができます。ただし、これは必ずしも「意味的に近い」誤訳を生成するとは限らない点に注意が必要です。

### 敵対的学習
これはより高度なアプローチで、モデルが意図的に「誤った」翻訳を生成するように訓練します。例えば、一つのモデルが正確な翻訳を生成し、別のモデルがその翻訳を「妨害」するように訓練することができます。しかし、この方法は実装が複雑で、多くの調整が必要になる可能性があります。
### 考察
正解を生成する可能性はあるが、基本的には誤った回答を出す様なものを作るのが無難そう
要するには、かなりの頻度で近いが誤った回答を出すモデルを作成する感じ
##　メモ
ニューラルネットワークの種類って何がある?
- CNN（畳み込みニューラルネットワーク）
  - もともと画像処理に使用されるために設計されたらしい
  - 一次元のCNNは自然言語処理でも使えるみたい
  - 特に、局所的な特徴を抽出する能力に優れていて、単語の近傍情報を利用するタスク（例えば、感情分析や名前付きエンティティ認識）に有用らしい
- RNN（再帰型ニューラルネットワーク）
  - シーケンス(時系列)データの処理の特化していて、過去の情報を元に現在の状況を予想する感じ
  - 自然言語はシーケンス（単語や文字の列）として表現されるみたいなので、自然言語処理でRNNはよく使われていたらしい
- LSTM（長・短期記憶）
  -  RNNの一種で、長期的な依存関係を学習できて、長いテキストシーケンス（例えば、文章や文書）の理解に有用らしい
  -  昔の情報を適度に忘れらたりすることで、  RNNの時にあった問題の、過去の情報がうまく伝わらないことを減らしたらしい
  -  短期に覚えるものと、長期で覚えるもので区別して覚える
  -  どれくらい忘れるかと、どれくらい覚えるかと、どれくらい反映させるかとかでパラメータが多いから計算負荷が大きいらしい
  -  長いシーケンスの場合でも情報の損失が発生するみたいで、近年のTransformerベースのモデルで部分的に解決されてるらしい
- GRU（ゲート付きリカレントユニット）
  - RNNの一種で、シーケンスの長さがそれほど長くない場合には、LSTMと同等の性能を発揮しながらも、計算コストを抑えることができるらしい
  - 長期保存の記憶と短期保存の記憶の両方を一つの場所で保持する
- Transformerベースのモデル
  - "Attention Mechanism"と呼ばれるテクニックを用いて、入力シーケンス内のすべての要素間の依存関係を同時に考慮してるらしい
  - シーケンス内のどの部分が他の部分と関連しているかを捉えることができるみたい

Word2Vec, FastTextて何?
- 単語のベクトル表現（単語埋め込み）を生成するためのアルゴリズム
- 単語の意味的・文法的な類似性をベクトル空間内に捉えることができるらしい
- 単純なフィードフォワードニューラルネットワークを使用してて、時系列や文脈情報を考慮する能力はないらしい

Seq2Seqって何？
- 一般的な設計パターンで、あるシーケンス（例えば、文章、音声、映像など）を別のシーケンス（翻訳された文章、音声からテキストへの変換、映像の説明文など）に変換する変換するために使用されるらしい
- Seq2Seqモデルは、RNNやLSTMやGRUに基づいて構築されることが多かった
- Googleの"T5"やFacebookの"BART"もSeq2Seqタスクに適用することが可できるらしい
- RNNでもトランスフォーマーでも実装可能であり、どちらの種類のモデルもSeq2Seqタスクを解くために使用できるらしい