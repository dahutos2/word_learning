{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47cf478-e158-4b71-904b-61cce20001d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab881fa4-ecc7-4647-91eb-e248a5dae4c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'JP28/t5-end2end-questions-generation'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'JP28/t5-end2end-questions-generation' is the correct path to a directory containing all relevant files for a T5Tokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 訓練が終了したモデルとトークナイザを保存\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# トークナイザとモデルの準備\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJP28/t5-end2end-questions-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJP28/t5-end2end-questions-generation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output/model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/leanrning_word_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1796\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1790\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1791\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 1796\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1800\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1801\u001b[0m     )\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'JP28/t5-end2end-questions-generation'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'JP28/t5-end2end-questions-generation' is the correct path to a directory containing all relevant files for a T5Tokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "# 訓練が終了したモデルとトークナイザを保存\n",
    "# トークナイザとモデルの準備\n",
    "tokenizer = T5Tokenizer.from_pretrained('JP28/t5-end2end-questions-generation', model_max_length=128)\n",
    "model = T5ForConditionalGeneration.from_pretrained('JP28/t5-end2end-questions-generation')\n",
    "\n",
    "model.save_pretrained(\"./output/model\")\n",
    "tokenizer.save_pretrained('./output/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce699788-00f1-4043-90f7-6c8a4b1528d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/okumuradaichi/opt/anaconda3/envs/leanrning_word_env/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 04:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=57, training_loss=14.902842739172149, metrics={'train_runtime': 259.4881, 'train_samples_per_second': 0.879, 'train_steps_per_second': 0.22, 'total_flos': 8677649940480.0, 'train_loss': 14.902842739172149, 'epoch': 3.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WordTranslationDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        english_word = row['English']\n",
    "        wrong_translation = row['Japanese']\n",
    "        inputs = self.tokenizer.encode_plus(english_word, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        outputs = self.tokenizer.encode_plus(wrong_translation, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': outputs['input_ids'].flatten(),\n",
    "        }\n",
    "\n",
    "# データの読み込みと前処理\n",
    "# 英単語と誤った日本語訳のペアのcsv\n",
    "df = pd.read_csv('./input/en_and_ja.csv')\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# トークナイザとモデルの準備\n",
    "tokenizer = T5Tokenizer.from_pretrained('./output/model')\n",
    "model = T5ForConditionalGeneration.from_pretrained('./output/model')\n",
    "\n",
    "# データセットの作成\n",
    "train_dataset = WordTranslationDataset(tokenizer, train_df)\n",
    "eval_dataset = WordTranslationDataset(tokenizer, eval_df)\n",
    "\n",
    "# トレーニングの設定\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    #モデルのチェックポイントやトレーニングの結果などが保存されるディレクトリです。\n",
    "    #異なる実験結果を区別するために、実行ごとに一意なディレクトリを指定することが重要です。\n",
    "    output_dir='./output/results',  \n",
    "    \n",
    "    #トレーニングのエポック数を指定します。モデルがデータセット全体を何回トレーニングするかを制御します。\n",
    "    #適切なエポック数は、データセットのサイズやタスクに依存します。\n",
    "    #過剰なトレーニングは過学習を引き起こす可能性があります。\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    #レーニングおよび評価時のデバイスごとのバッチサイズを指定します。\n",
    "    #モデルとGPUメモリの制約に合わせて適切なバッチサイズを選択する必要があります。\n",
    "    #メモリが不足する場合は、バッチサイズを減らす必要があります。\n",
    "    per_device_train_batch_size=32,  \n",
    "    per_device_eval_batch_size=16,\n",
    "    \n",
    "    #オプティマイザのウォームアップステップ数を指定します。\n",
    "    #ウォームアップステップは、学習率を線形に増加させるために使用されるステップ数です。\n",
    "    #ウォームアップにより、初期の学習が安定化し、最適な学習率に到達するのを助けます。\n",
    "    warmup_steps=500, \n",
    "    \n",
    "    #重みの減衰（L2正則化）の係数を指定します。\n",
    "    #重みの減衰は、過学習を防ぐために使用されます。値が大きいほど正則化の強度が高くなります。\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    #学習率は、最適化アルゴリズムがモデルの重みを更新する際に使用するステップサイズを制御します。\n",
    "    #学習率が小さすぎると収束が遅くなり、学習率が大きすぎると発散する可能性があります。\n",
    "    #適切な学習率を選択することは、モデルの収束性と最終的な性能に重要な影響を与えます。\n",
    "    learning_rate=5e-5,\n",
    "    \n",
    "    #勾配アキュムレーションは、バッチサイズを制限されたメモリリソースの中で増やすための手法です。\n",
    "    #通常、バッチサイズを大きくすると、GPUメモリが不足する場合があります。\n",
    "    #勾配アキュムレーションでは、複数の小さいバッチを逐次的に処理し、\n",
    "    #その勾配を一定のステップ数ごとに累積することで、大きなバッチサイズの効果を得ることができます。\n",
    "    #gradient_accumulation_stepsは、何ステップごとに勾配を累積するかを指定します。\n",
    "    gradient_accumulation_steps=1,\n",
    "    \n",
    "    #ログファイルが保存されるディレクトリです。\n",
    "    #トレーニングの進捗やメトリクスなどが記録されます。\n",
    "    logging_dir='./output/logs',        \n",
    ")\n",
    "\n",
    "### GPUの利用有無\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "if USE_GPU:\n",
    "    model.cuda()\n",
    "    \n",
    "# トレーナーの作成\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# 訓練\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6dacd-ba6e-4d49-a161-17a0cbd21921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練が終了したモデルとトークナイザを保存\n",
    "model.save_pretrained(\"./output/model\")\n",
    "tokenizer.save_pretrained('./output/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3b8285-e101-4a7e-acb0-e2e9faba822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザとモデルの準備\n",
    "tokenizer = T5Tokenizer.from_pretrained('./output/model')\n",
    "model = T5ForConditionalGeneration.from_pretrained('./output/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e05182-2a68-45ae-b601-f1300d38ee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a577c79-cf29-49a4-890c-7ba1eca18983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>.<extra_id_3> selfishness<extra_id_4> selfishness<extra_id_5>selfishness. selfishness selfishness\n",
      "<pad><extra_id_0>.<extra_id_1> incorporate<extra_id_2> incorporate into your company<extra_id_3>incorporate into your company incorporate into your company incorporate into\n",
      "<pad><extra_id_0> quarryman<extra_id_1>quarryman, quarryman, quarryman, quarryman, quarryman, quarry\n",
      "<pad><extra_id_0>.<extra_id_1> hobbyists<extra_id_2>hobbyists hobbyists hobbyists hobbyists hobby\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>.<extra_id_3>.<extra_id_4>greetings from the greetings card. greetings from\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>state of the art state of the art state of the art state of the\n",
      "<pad><extra_id_0>condense condense condense condense condense condense con\n",
      "<pad><extra_id_0>I can’t wait to see what you can do to help. I can’t wait\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>. Connect. Connect. Connect. Connect. Connect. Connect. Connect.\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>.<extra_id_3> massive<extra_id_4> massive<extra_id_5>massive massive massive massive massive massive massive massive massive\n",
      "<pad><extra_id_0>.<extra_id_1>inconsequentially. Inconsequentially inconsequent\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>.<extra_id_3> appointment<extra_id_4> appointment<extra_id_5>appointment appointment appointment appointment appointment appointment appointment appointment appointment\n",
      "<pad><extra_id_0> _degree<extra_id_1> _degree<extra_id_2> _degree<extra_id_3> _degree<extra_id_4> _degree<extra_id_5>_degree associ\n",
      "<pad>biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy biopsy\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>. worthy. worthy. worthy. worthy. worthy. worthy. worthy.\n",
      "<pad><extra_id_0>.<extra_id_1> rotation<extra_id_2>rotation. rotation. rotation. rotation. rotation. rotation. rotation. rotation\n",
      "<pad><extra_id_0> fake<extra_id_1> counterfeits<extra_id_2> counterfeits<extra_id_3> counterfeits<extra_id_4> s<extra_id_5>s and counterfeits are counterfeit\n",
      "<pad><extra_id_0>Typhaceae Typhaceae Typhaceae Typhaceae Typhacea\n",
      "<pad><extra_id_0>_up tie_up tie_up tie_up tie_up tie_up tie_\n",
      "<pad>brush brush brush brush brush brush brush brush brush brush brush brush brush brush brush brush brush brush brush brush\n",
      "<pad><extra_id_0>Slavs. Slavs. Slavs. Slavs\n",
      "<pad><extra_id_0>_patch_soul_patch_patch_soul_patch_\n",
      "<pad><extra_id_0> s<extra_id_1> s<extra_id_2> s<extra_id_3> aggrades<extra_id_4>s aggrades\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>.<extra_id_3> toughly. Hardly.</s>\n",
      "<pad>( ( ( ( ( ( poise poise poise poise poise poise poise poise poise poise poise poise poise poise\n",
      "<pad><extra_id_0> bosom<extra_id_1>bosom bosom bosom bosom bosom bosom bosom bosom\n",
      "<pad><extra_id_0>_glass_glass_watch_glass_watch_glass_watch_glass_watch_\n",
      "<pad><extra_id_0> adjacent<extra_id_1> adjacent<extra_id_2> adjacent<extra_id_3> adjacent adjacent<extra_id_4> to each other.<extra_id_5> adjacent<extra_id_6>to each other\n",
      "<pad><extra_id_0>osis, laryngosis, laryngosis, la\n",
      "<pad><extra_id_0> register<extra_id_1> register now! Register now! Register now! Register now! Register now!</s>\n",
      "<pad><extra_id_0>,<extra_id_1>and smugly. Smugly and smugly. Smug\n",
      "<pad><extra_id_0> fusion<extra_id_1>fusion fusion fusion fusion fusion fusion fusion fusion\n",
      "<pad><extra_id_0>dorsum dorsum dorsum dorsum dorsum dorsum do\n",
      "<pad><extra_id_0> diffuse<extra_id_1> diffuse<extra_id_2> diffuse diffuse<extra_id_3> diffuse diffuse diffuse<extra_id_4>diffuse diffuse diffuse diffuse diffuse diffuse diffuse diffuse\n",
      "<pad><extra_id_0> entire<extra_id_1> entire<extra_id_2> entire<extra_id_3> entire<extra_id_4> entire<extra_id_5> entire<extra_id_6> entire<extra_id_7> entire<extra_id_8> entire<extra_id_9>entire\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2> derive from the derive. derive.</s>\n",
      "<pad><extra_id_0> seduce<extra_id_1>seduce. seduce seduce seduce seduce seduce seduce se\n",
      "<pad><extra_id_0>phalocele. Encephalocele encephalocele encephalocele\n",
      "<pad>ritual ritual ritual ritual ritual ritual ritual ritual ritual ritual ritual ritual ritual ritual ritual ritual ritual ritual ritual ritual\n",
      "<pad>cut cut cut cut cut cut cut cut cut cut cut cut cut cut cut cut cut cut cut cut\n",
      "<pad><extra_id_0>.<extra_id_1> equilibrium<extra_id_2> equilibrium. equilibrium.<extra_id_3>equilibrium. equilibrium. equilibrium. equilibrium. equilibrium.\n",
      "<pad><extra_id_0>_of_the_way_of_the_way_of_the_way_\n",
      "<pad><extra_id_0>,<extra_id_1>,<extra_id_2>,<extra_id_3> coiled and<extra_id_4>,<extra_id_5>,<extra_id_6>,<extra_id_7>,<extra_id_8>,\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>.<extra_id_3>.<extra_id_4> fell victim to<extra_id_5>fell victim to fell victim to fell\n",
      "<pad><extra_id_0> s payables.</s>\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2> emblem<extra_id_3>emblem emblem emblem emblem emblem emblem emblem emblem emblem emblem emblem emblem emblem\n",
      "<pad><extra_id_0>dentine dentine dentine dentine dentine dentine dentine dentine dentine den\n",
      "<pad><extra_id_0> and<extra_id_1>,<extra_id_2> and<extra_id_3> slender. Slender and<extra_id_4>. Sl\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>.<extra_id_3>through to the end of the year. Through to the end of\n",
      "<pad><extra_id_0> ometric trigonometric<extra_id_1>ometric trigonometric trigonometric trigon\n",
      "<pad><extra_id_0>,<extra_id_1>, aimlessly aimlessly aimlessly aimlessly aimlessly aimlessly aimlessly aimlessly\n",
      "<pad><extra_id_0> still<extra_id_1>. Still<extra_id_2> still<extra_id_3> still<extra_id_4> are<extra_id_5>. Still<extra_id_6>a bit of\n",
      "<pad><extra_id_0>.<extra_id_1> Indonesian<extra_id_2> n Indonesian<extra_id_3>n Indonesian Indonesian Indonesian Indonesian Indonesia\n",
      "<pad><extra_id_0> Mistrial<extra_id_1>. Mistrial. Mistrial. Mistrial. Mist\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2> likeness<extra_id_3> likeness likeness<extra_id_4>. Likeness likeness is\n",
      "<pad>tap tap tap tap tap tap tap tap tap tap tap tap tap tap tap tap tap tap tap tap\n",
      "<pad><extra_id_0>_door_door_barn_door_door_barn_door_door_\n",
      "<pad><extra_id_0>_Forces_Special_Forces_Forces_For_Cas\n",
      "<pad><extra_id_0>_fever_rheumatic_fever_rheumatic_fever\n",
      "<pad><extra_id_0> nonverbal<extra_id_1> nonverbal. Nonverbal. Nonverbal.</s>\n",
      "<pad><extra_id_0> proboscis<extra_id_1>is proboscis proboscis probo\n",
      "<pad><extra_id_0> screwdriver<extra_id_1>. Uncrew the screwdriver and unscrew the screwdrive\n",
      "<pad><extra_id_0>_industry_chemical_industry_chemical_industry_chemical_\n",
      "<pad><extra_id_0>esophageal esophageal esophageal\n",
      "<pad>_side_view_side_view_view_side_view_view_side_\n",
      "<pad><extra_id_0> and<extra_id_1> of the original<extra_id_2> and<extra_id_3>.<extra_id_4> and<extra_id_5>. Reappraisal and<extra_id_6>\n",
      "<pad>Eviscerate eviscerate eviscerate eviscerate\n",
      "<pad><extra_id_0> pigs and<extra_id_1> s. pigs and pigs are<extra_id_2> s and<extra_id_3>\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>.<extra_id_3>.<extra_id_4> writing<extra_id_5> writing<extra_id_6> writing<extra_id_7> writing<extra_id_8> writing writing<extra_id_9>\n",
      "<pad><extra_id_0> startle<extra_id_1>le startle startle startle startle startle startle startle start\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>.<extra_id_3> engaged in engagements<extra_id_4>engaged in engagements engaged in engagements\n",
      "<pad>prote proteo proteo prote proteo prote prote prote prote prote prote prote prote prote prote prote prote\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>.<extra_id_3>comfort. comfort. comfort. comfort. comfort. comfort. comfort\n",
      "<pad><extra_id_0> track.com.au<extra_id_1>track.com.au. Racetrack.com racetrack race\n",
      "<pad><extra_id_0>wreck. Shipwreck. Shipwreck. Shipwreck.\n",
      "<pad><extra_id_0>-shaped saddle-shaped saddle-shaped saddle-shaped saddle-shaped saddle-shaped saddle-\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2> doer.<extra_id_3> doer.<extra_id_4>.<extra_id_5>.<extra_id_6>.\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2> embroil<extra_id_3> embroil<extra_id_4>.<extra_id_5> oil<extra_id_6>\n",
      "<pad><extra_id_0> is<extra_id_1> is sporidiosis<extra_id_2> is<extra_id_3> is<extra_id_4> is<extra_id_5> is<extra_id_6> is<extra_id_7>\n",
      "<pad><extra_id_0>_horse_horse_horse_hors\n",
      "<pad><extra_id_0> and<extra_id_1>.<extra_id_2> and<extra_id_3> to<extra_id_4> to accrete. accrete.<extra_id_5>\n",
      "<pad><extra_id_0> and<extra_id_1>. Thuggery. Thuggery. Thuggery. Thuggery.\n",
      "<pad><extra_id_0> and<extra_id_1>. Doctrinal. Doctrinal. Doctrinal.doc\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>.<extra_id_3>.<extra_id_4>.<extra_id_5>.<extra_id_6>.<extra_id_7>.<extra_id_8>.<extra_id_9>.\n",
      "<pad><extra_id_0>,<extra_id_1>,<extra_id_2> and<extra_id_3>ened. Unawakened, unawakened, un\n",
      "<pad><extra_id_0> c<extra_id_1> c<extra_id_2> c, coeliac,<extra_id_3> c, coeliac<extra_id_4>c,\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2> proves to be a reliable and reliable source of information.</s>\n",
      "<pad><extra_id_0> and<extra_id_1>of this act. Enactment of this act. Enactment of this act\n",
      "<pad><extra_id_0> <extra_id_1> agers.com<extra_id_2>foragers.com foragers foragers.\n",
      "<pad><extra_id_0>.<extra_id_1>.<extra_id_2>. flush flush flush flush flush flush flush flush flush flush flush flush flush flush\n",
      "<pad><extra_id_0> the<extra_id_1>.<extra_id_2>.<extra_id_3>truncate. truncate. trun\n",
      "<pad>- go_out_out_of_sa_sa_sa_s\n",
      "<pad><extra_id_0>connivance. Connivance. Connivance. Connivance\n",
      "<pad><extra_id_0> bond bond<extra_id_1> bond bond<extra_id_2> bond bond bond<extra_id_3>bond bond bond bond bond bond bond bond bond\n",
      "<pad><extra_id_0> nonlinear<extra_id_1>nonlinearity. Nonlinearity is nonlinearity of\n",
      "<pad><extra_id_0> ism<extra_id_1> ism<extra_id_2> ism is a metamorphosis of<extra_id_3>is\n",
      "Average edit distance: 99.41666666666667\n"
     ]
    }
   ],
   "source": [
    "# モデルが正しい日本語訳とどれだけ異なるかを評価する\n",
    "def evaluate_model(model, tokenizer, df):\n",
    "    model.eval()  # evaluation mode\n",
    "    total_distance = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        english_word = row['English']\n",
    "        correct_translation = row['Japanese']\n",
    "        inputs = tokenizer.encode_plus(english_word, return_tensors='pt')\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "        generated_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        total_distance += distance(correct_translation, generated_translation)\n",
    "    \n",
    "    average_distance = total_distance / len(df)\n",
    "    return average_distance\n",
    "\n",
    "eval_df = pd.read_csv('./input/en_and_ja.csv')\n",
    "average_distance = evaluate_model(model, tokenizer, eval_df)\n",
    "print(f\"Average edit distance: {average_distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e19fd-44f6-42fd-88f3-01d64b73d1af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leanrning_word_env",
   "language": "python",
   "name": "leanrning_word_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
